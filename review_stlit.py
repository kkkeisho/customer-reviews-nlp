# app.py
# Streamlit demo: 100 dummy hotel reviews -> aspect (multi-label) + sentiment -> dashboard
#
# Run:
#   pip install streamlit pandas numpy
#   # Optional (heavy): pip install transformers torch
#   streamlit run app.py
#
# Notes:
# - Default mode uses a lightweight keyword-based classifier (fast, no model download).
# - Optional mode uses HuggingFace zero-shot (BART MNLI) + sentiment pipeline (can be slow / needs model download).
# - Alerting uses counts/rates (NOT zero-shot probabilities).

import re
import random
from dataclasses import dataclass
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
import streamlit as st

# ----------------------------
# 1) Config
# ----------------------------
ASPECTS = [
    "cleanliness",
    "staff",
    "facility",
    "noise",
    "location",
    "price/value",
    "food",
    "checkin/checkout",
    "wifi",
    "other",
]

# Keyword dictionary for lightweight aspect detection (edit freely)
ASPECT_KEYWORDS = {
    "cleanliness": ["clean", "dirty", "smell", "stain", "hair", "bathroom", "mold", "dust", "towel"],
    "staff": ["staff", "rude", "friendly", "helpful", "unhelpful", "service", "receptionist", "manager"],
    "facility": ["room", "bed", "aircon", "ac", "heater", "shower", "elevator", "gym", "pool", "facility", "amenities"],
    "noise": ["noisy", "noise", "loud", "thin walls", "neighbors", "street noise", "construction"],
    "location": ["location", "near", "far", "walk", "subway", "station", "central", "downtown", "airport"],
    "price/value": ["price", "value", "expensive", "cheap", "worth", "overpriced", "cost"],
    "food": ["breakfast", "food", "restaurant", "coffee", "buffet", "dinner"],
    "checkin/checkout": ["check-in", "checkin", "check out", "checkout", "queue", "line", "waiting", "front desk"],
    "wifi": ["wifi", "wi-fi", "internet", "connection", "signal"],
}

# Keyword dictionary for lightweight sentiment
POS_WORDS = ["great", "good", "excellent", "amazing", "clean", "comfortable", "friendly", "helpful", "fast", "quiet"]
NEG_WORDS = ["bad", "terrible", "awful", "dirty", "rude", "slow", "noisy", "smell", "broken", "worst", "forever"]

SOURCES = ["Booking.com", "Expedia", "Google", "Agoda", "Tripadvisor"]
LANGS = ["en", "en", "en", "en", "ja"]  # mostly English, some Japanese for realism


# ----------------------------
# 2) Dummy review generator
# ----------------------------
@dataclass
class DummyReview:
    hotel_id: str
    source: str
    lang: str
    date: datetime
    rating: int
    text: str


def _pick_phrase(aspect: str, sentiment: str) -> str:
    # short phrase templates per aspect
    templates = {
        "cleanliness": {
            "pos": ["The room was very clean.", "Bathroom was spotless.", "Fresh towels and no smell."],
            "neg": ["The room was not clean.", "There was a bad smell in the bathroom.", "Found hair on the bed."],
        },
        "staff": {
            "pos": ["Staff were friendly and helpful.", "Reception was very kind.", "Service was excellent."],
            "neg": ["Staff was rude.", "Receptionist was unhelpful.", "Customer service was terrible."],
        },
        "facility": {
            "pos": ["Bed was comfortable.", "Facilities were great.", "Shower pressure was good."],
            "neg": ["Aircon was broken.", "Elevator was slow and old.", "The room felt outdated."],
        },
        "noise": {
            "pos": ["It was quiet at night.", "No noise issues.", "Soundproofing was decent."],
            "neg": ["Very noisy at night.", "Thin walls, could hear neighbors.", "Street noise was loud."],
        },
        "location": {
            "pos": ["Great location near the station.", "Easy walk to downtown.", "Convenient area."],
            "neg": ["Location was far from everything.", "Hard to access without a car.", "Not a convenient area."],
        },
        "price/value": {
            "pos": ["Good value for the price.", "Worth the cost.", "Reasonably priced."],
            "neg": ["Overpriced for what you get.", "Not worth the price.", "Too expensive."],
        },
        "food": {
            "pos": ["Breakfast was tasty.", "Good buffet options.", "Nice coffee and food."],
            "neg": ["Breakfast was disappointing.", "Food quality was bad.", "Limited options at the restaurant."],
        },
        "checkin/checkout": {
            "pos": ["Check-in was smooth and quick.", "No waiting at the front desk.", "Checkout was easy."],
            "neg": ["The check-in took forever.", "Long queue at reception.", "Checkout process was slow."],
        },
        "wifi": {
            "pos": ["WiFi was fast and stable.", "Internet worked perfectly.", "Strong signal in the room."],
            "neg": ["WiFi was terrible.", "Internet kept disconnecting.", "Weak signal in the room."],
        },
        "other": {
            "pos": ["Overall a great stay.", "Would come again.", "Loved the experience."],
            "neg": ["Overall disappointing.", "Would not stay again.", "Not recommended."],
        },
    }
    return random.choice(templates.get(aspect, templates["other"])[sentiment])


def generate_dummy_reviews(n: int = 100, seed: int = 42) -> list[DummyReview]:
    random.seed(seed)
    np.random.seed(seed)

    hotels = [f"H{str(i).zfill(3)}" for i in range(1, 6)]  # 5 hotels
    start = datetime.now() - timedelta(days=90)

    reviews: list[DummyReview] = []
    for _ in range(n):
        hotel_id = random.choice(hotels)
        source = random.choice(SOURCES)
        lang = random.choice(LANGS)
        date = start + timedelta(days=int(np.random.randint(0, 90)))

        # Choose 1-3 aspects
        k = int(np.random.choice([1, 2, 3], p=[0.45, 0.40, 0.15]))
        aspects = random.sample(ASPECTS[:-1], k=k)  # exclude "other" for selection; we can add later
        # Decide overall sentiment bias
        overall = np.random.choice(["pos", "neg"], p=[0.62, 0.38])

        # Create text by combining aspect phrases + occasional "other"
        phrases = [_pick_phrase(a, overall) for a in aspects]
        if np.random.rand() < 0.25:
            phrases.append(_pick_phrase("other", overall))

        text_en = " ".join(phrases)

        # Quick Japanese flavor (simple)
        if lang == "ja":
            text = text_en.replace("Staff was rude.", "ã‚¹ã‚¿ãƒƒãƒ•ãŒå¤±ç¤¼ã§ã—ãŸã€‚").replace(
                "The room was very clean.", "éƒ¨å±‹ã¯ã¨ã¦ã‚‚æ¸…æ½”ã§ã—ãŸã€‚"
            )
        else:
            text = text_en

        # Rating correlated with overall
        rating = int(np.random.choice([5, 4, 3, 2, 1], p=[0.35, 0.27, 0.18, 0.12, 0.08])) if overall == "pos" else int(
            np.random.choice([5, 4, 3, 2, 1], p=[0.05, 0.10, 0.20, 0.30, 0.35])
        )

        reviews.append(DummyReview(hotel_id, source, lang, date, rating, text))
    return reviews


# ----------------------------
# 3) Lightweight classifiers
# ----------------------------
def keyword_aspect_scores(text: str, labels: list[str]) -> dict[str, float]:
    t = text.lower()
    scores = {}
    for lab in labels:
        if lab == "other":
            continue
        hits = 0
        for kw in ASPECT_KEYWORDS.get(lab, []):
            if kw in t:
                hits += 1
        # soft score: diminishing returns
        scores[lab] = 1 - np.exp(-hits / 2)  # 0..~1
    # other score: inverse of max hit (if nothing matches => higher "other")
    max_hit = max(scores.values()) if scores else 0.0
    scores["other"] = float(max(0.0, 0.6 - max_hit))
    return scores


def keyword_sentiment(text: str) -> tuple[str, float]:
    t = text.lower()
    pos = sum(1 for w in POS_WORDS if w in t)
    neg = sum(1 for w in NEG_WORDS if w in t)
    if pos == 0 and neg == 0:
        return ("NEUTRAL", 0.50)
    if neg > pos:
        conf = min(0.55 + (neg - pos) * 0.10, 0.99)
        return ("NEGATIVE", float(conf))
    if pos > neg:
        conf = min(0.55 + (pos - neg) * 0.10, 0.99)
        return ("POSITIVE", float(conf))
    return ("NEUTRAL", 0.55)


# ----------------------------
# 4) Optional Transformers pipelines
# ----------------------------
@st.cache_resource
def load_transformers_pipelines():
    from transformers import pipeline

    zs = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
    sent = pipeline("sentiment-analysis")  # default model
    return zs, sent


def classify_review(
    text: str,
    labels: list[str],
    mode: str,
    top_k: int,
    min_score: float,
) -> dict:
    """
    Returns:
      {
        "sentiment_label": ...,
        "sentiment_score": ...,
        "labels": [..sorted..],
        "scores": [..],
        "selected": [(label, score), ...]
      }
    """
    if mode == "Transformers (zero-shot + sentiment)":
        zs, sent = load_transformers_pipelines()
        out_zs = zs(text, labels)
        out_sent = sent(text)[0]
        ranked = list(zip(out_zs["labels"], out_zs["scores"]))
    else:
        # lightweight
        aspect_scores = keyword_aspect_scores(text, labels)
        ranked = sorted(aspect_scores.items(), key=lambda x: x[1], reverse=True)
        s_lab, s_score = keyword_sentiment(text)
        out_sent = {"label": s_lab, "score": s_score}

    selected = [(lab, float(sc)) for lab, sc in ranked[:top_k] if sc >= min_score]

    return {
        "sentiment_label": out_sent["label"],
        "sentiment_score": float(out_sent["score"]),
        "labels": [lab for lab, _ in ranked],
        "scores": [float(sc) for _, sc in ranked],
        "selected": selected,
    }


def to_aspect_events(df_reviews: pd.DataFrame, col_selected: str = "selected") -> pd.DataFrame:
    rows = []
    for _, r in df_reviews.iterrows():
        for (aspect, conf) in r[col_selected]:
            rows.append(
                {
                    "hotel_id": r["hotel_id"],
                    "review_id": r["review_id"],
                    "date": r["date"],
                    "source": r["source"],
                    "lang": r["lang"],
                    "rating": r["rating"],
                    "aspect": aspect,
                    "aspect_confidence": float(conf),
                    "sentiment": r["sentiment_label"],
                    "sentiment_confidence": float(r["sentiment_score"]),
                    "text": r["text"],
                }
            )
    return pd.DataFrame(rows)


# ----------------------------
# 5) Streamlit UI
# ----------------------------
st.set_page_config(page_title="Hotel Review NLP Dashboard (Dummy)", layout="wide")

st.title("ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»•åˆ†ã‘ï¼ˆNLPï¼‰ãƒ‡ãƒ¢ï¼š100ä»¶ãƒ€ãƒŸãƒ¼ãƒ¬ãƒ“ãƒ¥ãƒ¼ â†’ ã‚¢ã‚¹ãƒšã‚¯ãƒˆÃ—æ„Ÿæƒ… â†’ æ”¹å–„ã«ç¹‹ãŒã‚‹å¯è¦–åŒ–")

with st.sidebar:
    st.header("è¨­å®š")
    seed = st.number_input("ä¹±æ•°ã‚·ãƒ¼ãƒ‰", min_value=0, max_value=10_000, value=42, step=1)
    n = st.slider("ãƒ€ãƒŸãƒ¼ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»¶æ•°", 20, 500, 100, 10)

    mode = st.selectbox(
        "åˆ†é¡ãƒ¢ãƒ¼ãƒ‰",
        ["Lightweight (keyword)", "Transformers (zero-shot + sentiment)"],
        index=0,
        help="Transformersã¯åˆå›ã«ãƒ¢ãƒ‡ãƒ«DLãŒå¿…è¦ã§é‡ã„ã§ã™ã€‚",
    )
    top_k = st.slider("ã‚¢ã‚¹ãƒšã‚¯ãƒˆTop-Kæ¡ç”¨", 1, 5, 3, 1)
    min_score = st.slider("ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¡ç”¨ã®æœ€ä½ã‚¹ã‚³ã‚¢é–¾å€¤", 0.0, 0.8, 0.10, 0.01)
    st.caption("â€» zero-shotã®scoreã¯â€œç¢ºç‡â€ã§ã¯ãªã„ã®ã§ã€é‹ç”¨ã¯ä»¶æ•°/ç‡ã§è¦‹ã‚‹ã®ãŒå®‰å…¨ã§ã™ã€‚")

    st.divider()
    st.subheader("ãƒ•ã‚£ãƒ«ã‚¿")
    hotel_filter = st.multiselect("ãƒ›ãƒ†ãƒ«", options=[f"H{str(i).zfill(3)}" for i in range(1, 6)], default=[])
    source_filter = st.multiselect("ã‚½ãƒ¼ã‚¹", options=SOURCES, default=[])
    sentiment_filter = st.multiselect("æ„Ÿæƒ…", options=["POSITIVE", "NEGATIVE", "NEUTRAL"], default=[])

    st.divider()
    st.subheader("ğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šï¼ˆé€±æ¬¡ï¼‰")
    lookback_weeks = st.slider("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆéå»Né€±ï¼‰", 2, 12, 4, 1)
    z_threshold = st.slider("é–¾å€¤ï¼ˆå¹³å‡ + zÃ—Ïƒï¼‰", 0.5, 4.0, 2.0, 0.1)
    min_events_week = st.slider("ä»Šé€±ã®æœ€ä½ã‚¤ãƒ™ãƒ³ãƒˆæ•°ï¼ˆæ¯æ•°ã‚¬ãƒ¼ãƒ‰ï¼‰", 5, 100, 20, 1)


@st.cache_data
def build_reviews_df(n: int, seed: int) -> pd.DataFrame:
    reviews = generate_dummy_reviews(n=n, seed=seed)
    df = pd.DataFrame(
        [
            {
                "review_id": f"R{idx:04d}",
                "hotel_id": r.hotel_id,
                "source": r.source,
                "lang": r.lang,
                "date": r.date.date(),
                "rating": r.rating,
                "text": r.text,
            }
            for idx, r in enumerate(reviews, start=1)
        ]
    )
    return df


df = build_reviews_df(n=n, seed=seed).copy()

# Apply filters
if hotel_filter:
    df = df[df["hotel_id"].isin(hotel_filter)]
if source_filter:
    df = df[df["source"].isin(source_filter)]

st.divider()
st.subheader("åˆ†æï¼šOTAã”ã¨ã®é¡§å®¢å±¤ã‚’è¦‹ã‚‹")
focus_source = st.selectbox(
    "ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ã—ãŸã„ã‚½ãƒ¼ã‚¹ï¼ˆä»»æ„ï¼‰",
    options=["(All)"] + SOURCES,
    index=0,
    help="ã“ã“ã§é¸ã¶ã¨ã€ãã®OTAã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã ã‘ä¸‹ã«è¡¨ç¤ºã—ã¾ã™ã€‚",
)

# Classify
with st.spinner("åˆ†é¡ä¸­..."):
    results = []
    for text in df["text"].tolist():
        results.append(classify_review(text, ASPECTS, mode, top_k, min_score))

df["sentiment_label"] = [r["sentiment_label"] for r in results]
df["sentiment_score"] = [r["sentiment_score"] for r in results]
df["ranked_labels"] = [r["labels"] for r in results]
df["ranked_scores"] = [r["scores"] for r in results]
df["selected"] = [r["selected"] for r in results]

if sentiment_filter:
    df = df[df["sentiment_label"].isin(sentiment_filter)]

events = to_aspect_events(df)

# ----------------------------
# 6) Dashboard
# ----------------------------
col1, col2, col3, col4 = st.columns(4)
col1.metric("ãƒ¬ãƒ“ãƒ¥ãƒ¼ä»¶æ•°", int(df.shape[0]))
col2.metric("ã‚¤ãƒ™ãƒ³ãƒˆä»¶æ•°ï¼ˆãƒ¬ãƒ“ãƒ¥ãƒ¼Ã—ã‚¢ã‚¹ãƒšã‚¯ãƒˆï¼‰", int(events.shape[0]))
neg_rate = (events["sentiment"].eq("NEGATIVE").mean() * 100) if len(events) else 0
col3.metric("NEGç‡ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆåŸºæº–ï¼‰", f"{neg_rate:.1f}%")
top_aspect = (
    events[events["sentiment"] == "NEGATIVE"]["aspect"].value_counts().head(1).index[0]
    if len(events) and (events["sentiment"] == "NEGATIVE").any()
    else "-"
)
col4.metric("æœ€å¤šNEGã‚¢ã‚¹ãƒšã‚¯ãƒˆ", top_aspect)

st.divider()

left, right = st.columns([1, 1])

with left:
    st.subheader("ã‚¢ã‚¹ãƒšã‚¯ãƒˆåˆ¥ï¼šä»¶æ•°ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆï¼‰")
    if len(events):
        aspect_counts = events["aspect"].value_counts().rename_axis("aspect").reset_index(name="count")
        st.bar_chart(aspect_counts.set_index("aspect"))
    else:
        st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã‚’ç·©ã‚ã¦ãã ã•ã„ï¼‰ã€‚")

with right:
    st.subheader("ã‚¢ã‚¹ãƒšã‚¯ãƒˆåˆ¥ï¼šNEGç‡ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆï¼‰")
    if len(events):
        grp = events.groupby("aspect")["sentiment"].apply(lambda s: (s == "NEGATIVE").mean()).reset_index(name="neg_rate")
        grp["neg_rate"] = grp["neg_rate"] * 100
        st.bar_chart(grp.set_index("aspect"))
    else:
        st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

st.divider()

# ----------------------------
# 7) Weekly Alerts (NEW)
# ----------------------------
st.subheader("ğŸš¨ é€±æ¬¡ã‚¢ãƒ©ãƒ¼ãƒˆï¼šã‚¢ã‚¹ãƒšã‚¯ãƒˆåˆ¥ NEGç‡ã®ç•°å¸¸æ¤œçŸ¥ï¼ˆãƒ›ãƒ†ãƒ«å…¨ä½“ï¼‰")
st.caption("è¨­è¨ˆæ€æƒ³ï¼šzero-shotã®scoreã¯ä½¿ã‚ãšã€ã‚¤ãƒ™ãƒ³ãƒˆä»¶æ•°ã¨NEGç‡ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã§æ¤œçŸ¥ã™ã‚‹ã€‚")

if len(events):
    ev = events.copy()
    ev["date"] = pd.to_datetime(ev["date"], errors="coerce")

    # Week start (Mon) as timestamp
    # Period('W') is typically week ending Sunday; start_time gives Monday start.
    ev["week"] = ev["date"].dt.to_period("W").apply(lambda p: p.start_time)

    weekly = (
        ev.groupby(["week", "aspect"])
        .agg(
            events_cnt=("review_id", "count"),
            neg_cnt=("sentiment", lambda s: (s == "NEGATIVE").sum()),
        )
        .reset_index()
    )
    weekly["neg_rate"] = weekly["neg_cnt"] / weekly["events_cnt"]

    # Show a quick trend table (optional but useful)
    with st.expander("é€±æ¬¡ã‚µãƒãƒªï¼ˆç¢ºèªç”¨ï¼‰", expanded=False):
        show_weekly = weekly.copy()
        show_weekly["neg_rate"] = (show_weekly["neg_rate"] * 100).round(1)
        st.dataframe(show_weekly.sort_values(["week", "aspect"], ascending=[False, True]), width="stretch")

    # Alert logic: current week vs lookback mean + z*std, with min sample guard
    alerts = []
    for aspect, g in weekly.groupby("aspect"):
        g = g.sort_values("week")
        if len(g) < (lookback_weeks + 1):
            continue  # not enough history

        curr = g.iloc[-1]
        hist = g.iloc[-(lookback_weeks + 1) : -1]

        mean = float(hist["neg_rate"].mean())
        std = float(hist["neg_rate"].std(ddof=0))

        if (
            int(curr["events_cnt"]) >= int(min_events_week)
            and std > 0
            and float(curr["neg_rate"]) > mean + float(z_threshold) * std
        ):
            alerts.append(
                {
                    "week": pd.Timestamp(curr["week"]).date(),
                    "aspect": aspect,
                    "neg_rate_%": round(float(curr["neg_rate"]) * 100, 1),
                    "baseline_%": round(mean * 100, 1),
                    "delta_pp": round((float(curr["neg_rate"]) - mean) * 100, 1),
                    "events_cnt": int(curr["events_cnt"]),
                }
            )

    alerts_df = pd.DataFrame(alerts)

    if len(alerts_df):
        st.dataframe(alerts_df.sort_values(["delta_pp"], ascending=False), use_container_width=True)

        st.markdown("**Evidenceï¼ˆä»Šé€±Ã—è©²å½“ã‚¢ã‚¹ãƒšã‚¯ãƒˆã®NEGä¾‹ï¼‰**")
        latest_week = ev["week"].max()

        for _, a in alerts_df.sort_values(["delta_pp"], ascending=False).iterrows():
            aspect = a["aspect"]
            subset = ev[
                (ev["week"] == latest_week)
                & (ev["aspect"] == aspect)
                & (ev["sentiment"] == "NEGATIVE")
            ].copy()

            subset = subset.sort_values(["aspect_confidence", "sentiment_confidence"], ascending=False)
            subset = subset.head(5)

            st.markdown(f"- **{aspect}**ï¼ˆä»Šé€±NEGç‡ {a['neg_rate_%']}% / é€šå¸¸ {a['baseline_%']}% / +{a['delta_pp']}pp, n={a['events_cnt']}ï¼‰")
            if len(subset):
                st.dataframe(subset[["date", "source", "rating", "text"]], use_container_width=True)
            else:
                st.info("è©²å½“ã™ã‚‹NEGä¾‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ãªã©ã‚’ç¢ºèªï¼‰ã€‚")
    else:
        st.success("ä»Šé€±ã®ã‚¢ãƒ©ãƒ¼ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ ğŸ‰ï¼ˆæ¡ä»¶ï¼šä»Šé€±nâ‰¥{0} & ä»Šé€±NEGç‡ > éå»{1}é€±å¹³å‡ + {2}Ïƒï¼‰".format(
            min_events_week, lookback_weeks, z_threshold
        ))
else:
    st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

st.divider()

st.subheader("ãƒ›ãƒ†ãƒ«Ã—ã‚¢ã‚¹ãƒšã‚¯ãƒˆï¼šNEGä»¶æ•°ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆè¡¨ï¼‰")
if len(events):
    pivot = (
        events[events["sentiment"] == "NEGATIVE"]
        .pivot_table(index="hotel_id", columns="aspect", values="review_id", aggfunc="count", fill_value=0)
        .astype(int)
    )
    st.dataframe(pivot, width='stretch')
else:
    st.info("NEGã‚¤ãƒ™ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

st.divider()

st.subheader("ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸€è¦§ï¼ˆåˆ†é¡çµæœã¤ãï¼‰")
show_cols = ["review_id", "hotel_id", "date", "source", "rating", "sentiment_label", "sentiment_score", "selected", "text"]
df_display = df[show_cols].copy()
df_display["selected"] = df_display["selected"].apply(lambda x: str(x) if x else "")
st.dataframe(df_display.sort_values(["date", "hotel_id"], ascending=[False, True]), width='stretch')

st.divider()

st.subheader("æ”¹å–„ã«ç›´çµã•ã›ã‚‹ï¼šæ ¹æ‹ ãƒ•ãƒ¬ãƒ¼ã‚ºå€™è£œï¼ˆç°¡æ˜“æŠ½å‡ºï¼‰")
st.caption("â€» æœ¬æ ¼çš„ã«ã¯aspectã”ã¨ã«è©²å½“æ–‡ã‚¹ãƒ‘ãƒ³æŠ½å‡º/è¦ç´„ã«ã™ã‚‹ã¨å¼·ã„ã€‚ã“ã“ã§ã¯é›°å›²æ°—ã®ãƒ‡ãƒ¢ã€‚")

if len(events):
    # pick negative events and extract a crude "evidence" snippet
    def evidence_snippet(txt: str, aspect: str) -> str:
        t = txt
        kws = ASPECT_KEYWORDS.get(aspect, [])
        for kw in kws:
            m = re.search(rf"(.{{0,40}}{re.escape(kw)}.{{0,40}})", t, flags=re.IGNORECASE)
            if m:
                return m.group(1).strip()
        return (t[:80] + "â€¦") if len(t) > 80 else t

    neg_events = events[events["sentiment"] == "NEGATIVE"].copy()
    if len(neg_events):
        neg_events["evidence"] = [
            evidence_snippet(t, a) for t, a in zip(neg_events["text"].tolist(), neg_events["aspect"].tolist())
        ]
        # top aspects evidence table
        topA = neg_events["aspect"].value_counts().head(3).index.tolist()
        tab = neg_events[neg_events["aspect"].isin(topA)][
            ["hotel_id", "date", "aspect", "aspect_confidence", "evidence", "source", "rating"]
        ].sort_values(["aspect", "aspect_confidence"], ascending=[True, False])
        st.dataframe(tab.head(50), width='stretch')
    else:
        st.success("NEGã‚¤ãƒ™ãƒ³ãƒˆãŒãªã„ã®ã§ã€æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯ä¸è¦ï¼ˆã“ã®ãƒ€ãƒŸãƒ¼æ¡ä»¶ã§ã¯ï¼‰")
else:
    st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
